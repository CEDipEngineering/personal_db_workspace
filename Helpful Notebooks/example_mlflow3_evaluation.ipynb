{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f719f8-4b4e-4230-ada3-95874b68a361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to MLFlow 3 Evaluation\n",
    "\n",
    "When it comes to GenAI performance, results can be subjective, complex, and difficult to analyze. This makes manual testing slow, and sometimes prohibitively expensive. The goal of MLFlow's Evaluation framework is making this process as automated and easy as possible, while still keeping a human in the loop.\n",
    "\n",
    "This notebook aims to introduce the concepts of MLflow 3 evaluation, demonstrating how to use the `mlflow.evaluate()` API to assess model performance, log metrics, and gain insights into model behavior.\n",
    "\n",
    "It is possible to achieve a very similar setup using only the MLFlow 3 UI within Databricks! Check out the documentation [here](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/evaluate-app?language=Use+the+UI) if you have any questions or issues, or just prefer to use the UI directly!\n",
    "\n",
    "This was made on 2025-07-11 using DBR 16.4 LTS ML, but should work just fine with Serverless or future versions of DBR. In this version, MLFlow 3.0 is not standard, and must be installed manually.\n",
    "\n",
    "## Index:\n",
    "1. Setup\n",
    "2. Evaluation\n",
    "3. Results \n",
    "\n",
    "___ \n",
    "## Setup\n",
    "In this section, we create functions to simulate a data retrieval mechanism (such as a Vector Search Index), some data structures to simulate your CRM tables, and a function to take the place of our Generative AI model we're looking to test, however in your scenario, you likely have most if not all of these ready-to-go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046dfc46-0431-40bd-b5cc-41fbcc5ad0a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow[databricks]>=3.1.0\" openai\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199466f1-482a-4d46-a698-377a91af4b75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Example"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from openai import OpenAI\n",
    "from mlflow.entities import Document\n",
    "from typing import List, Dict\n",
    "\n",
    "# Enable automatic tracing for OpenAI calls\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Connect to a Databricks LLM using OpenAI using the same credentials as MLflow\n",
    "# Alternatively, you can use your own OpenAI credentials here\n",
    "mlflow_creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "client = OpenAI(\n",
    "    api_key=mlflow_creds.token,\n",
    "    base_url=f\"{mlflow_creds.host}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Simulated customer relationship management database\n",
    "CRM_DATA = {\n",
    "    \"Acme Corp\": {\n",
    "        \"contact_name\": \"Alice Chen\",\n",
    "        \"recent_meeting\": \"Product demo on Monday, very interested in enterprise features. They asked about: advanced analytics, real-time dashboards, API integrations, custom reporting, multi-user support, SSO authentication, data export capabilities, and pricing for 500+ users\",\n",
    "        \"support_tickets\": [\"Ticket #123: API latency issue (resolved last week)\", \"Ticket #124: Feature request for bulk import\", \"Ticket #125: Question about GDPR compliance\"],\n",
    "        \"account_manager\": \"Sarah Johnson\"\n",
    "    },\n",
    "    \"TechStart\": {\n",
    "        \"contact_name\": \"Bob Martinez\",\n",
    "        \"recent_meeting\": \"Initial sales call last Thursday, requested pricing\",\n",
    "        \"support_tickets\": [\"Ticket #456: Login issues (open - critical)\", \"Ticket #457: Performance degradation reported\", \"Ticket #458: Integration failing with their CRM\"],\n",
    "        \"account_manager\": \"Mike Thompson\"\n",
    "    },\n",
    "    \"Global Retail\": {\n",
    "        \"contact_name\": \"Carol Wang\",\n",
    "        \"recent_meeting\": \"Quarterly review yesterday, happy with platform performance\",\n",
    "        \"support_tickets\": [],\n",
    "        \"account_manager\": \"Sarah Johnson\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use a retriever span to enable MLflow's predefined RetrievalGroundedness scorer to work\n",
    "# In a your use case, this could be a Vector Search Index, or some other tool that gathers data\n",
    "@mlflow.trace(span_type=\"RETRIEVER\")\n",
    "def retrieve_customer_info(customer_name: str) -> List[Document]:\n",
    "    \"\"\"Retrieve customer information from CRM database\"\"\"\n",
    "    if customer_name in CRM_DATA:\n",
    "        data = CRM_DATA[customer_name]\n",
    "        return [\n",
    "            Document(\n",
    "                id=f\"{customer_name}_meeting\",\n",
    "                page_content=f\"Recent meeting: {data['recent_meeting']}\",\n",
    "                metadata={\"type\": \"meeting_notes\"}\n",
    "            ),\n",
    "            Document(\n",
    "                id=f\"{customer_name}_tickets\",\n",
    "                page_content=f\"Support tickets: {', '.join(data['support_tickets']) if data['support_tickets'] else 'No open tickets'}\",\n",
    "                metadata={\"type\": \"support_status\"}\n",
    "            ),\n",
    "            Document(\n",
    "                id=f\"{customer_name}_contact\",\n",
    "                page_content=f\"Contact: {data['contact_name']}, Account Manager: {data['account_manager']}\",\n",
    "                metadata={\"type\": \"contact_info\"}\n",
    "            )\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "# This is your main application function. It uses your model and retriever and any other tools to generate an output based on the user's input.\n",
    "# If you are doing RAG, this is probably a function that calls your final LangChain chain for inference.\n",
    "@mlflow.trace\n",
    "def generate_sales_email(customer_name: str, user_instructions: str) -> Dict[str, str]:\n",
    "    \"\"\"Generate personalized sales email based on customer data & a sale's rep's instructions.\"\"\"\n",
    "    # Retrieve customer information\n",
    "    customer_docs = retrieve_customer_info(customer_name)\n",
    "\n",
    "    # Combine retrieved context\n",
    "    context = \"\\n\".join([doc.page_content for doc in customer_docs])\n",
    "\n",
    "    # Generate email using retrieved context\n",
    "    prompt = f\"\"\"You are a sales representative. Based on the customer information below,\n",
    "    write a brief follow-up email that addresses their request.\n",
    "\n",
    "    Customer Information:\n",
    "    {context}\n",
    "\n",
    "    User instructions: {user_instructions}\n",
    "\n",
    "    Keep the email concise and personalized.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"databricks-claude-3-7-sonnet\", # This example uses a Databricks hosted LLM - you can replace this with any AI Gateway or Model Serving endpoint. If you provide your own OpenAI credentials, replace with a valid OpenAI model e.g., gpt-4o, etc.\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful sales assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=2000\n",
    "    )\n",
    "\n",
    "    return {\"email\": response.choices[0].message.content}\n",
    "\n",
    "# Test the application\n",
    "result = generate_sales_email(\"Acme Corp\", \"Follow up after product demo\")\n",
    "print(result[\"email\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f658d35d-4a6b-4440-b3d9-8feab886bb52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simulate user interaction"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate beta testing traffic with scenarios designed to fail guidelines\n",
    "# In your use case, you would probably already have this data from initial testing\n",
    "test_requests = [\n",
    "    {\"customer_name\": \"Acme Corp\", \"user_instructions\": \"Follow up after product demo\"},\n",
    "    {\"customer_name\": \"TechStart\", \"user_instructions\": \"Check on support ticket status\"},\n",
    "    {\"customer_name\": \"Global Retail\", \"user_instructions\": \"Send quarterly review summary\"},\n",
    "    {\"customer_name\": \"Acme Corp\", \"user_instructions\": \"Write a very detailed email explaining all our product features, pricing tiers, implementation timeline, and support options\"},\n",
    "    {\"customer_name\": \"TechStart\", \"user_instructions\": \"Send an enthusiastic thank you for their business!\"},\n",
    "    {\"customer_name\": \"Global Retail\", \"user_instructions\": \"Send a follow-up email\"},\n",
    "    {\"customer_name\": \"Acme Corp\", \"user_instructions\": \"Just check in to see how things are going\"},\n",
    "]\n",
    "\n",
    "# Run requests and capture traces\n",
    "print(\"Simulating production traffic...\")\n",
    "for req in test_requests:\n",
    "    try:\n",
    "        result = generate_sales_email(**req)\n",
    "        print(f\"✓ Generated email for {req['customer_name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error for {req['customer_name']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecadcd3c-3e4d-474d-ae5d-411d448fa0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "___\n",
    "# Evaluation\n",
    "Here we create an evaluation dataset using some example questions that were submitted to our model as a baseline. If you want to know more about MLFlow's Evaluation Datasets, check out the [documentation] (https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/concepts/eval-datasets).\n",
    "\n",
    "After that, we create what are called [Scorers/Judges](https://docs.databricks.com/gcp/en/mlflow3/genai/eval-monitor/concepts/judges/). These are responsible for evaluating the answers provided by our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de33a9de-45fa-4f83-b047-8a95785b7a36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluation Dataset"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "\n",
    "# 1. Create an evaluation dataset\n",
    "# Replace with a Unity Catalog schema where you have CREATE TABLE permission\n",
    "uc_schema = \"main.default\"\n",
    "# This table will be created in the above UC schema\n",
    "evaluation_dataset_table_name = \"email_generation_eval\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {uc_schema}.{evaluation_dataset_table_name}\")\n",
    "eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "    uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\",\n",
    ")\n",
    "print(f\"Created evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}\")\n",
    "\n",
    "# 2. Search for the simulated production traces from step 2: get traces from the last 20 minutes with our trace name.\n",
    "ten_minutes_ago = int((time.time() - 10 * 60) * 1000)\n",
    "\n",
    "traces = mlflow.search_traces(\n",
    "    filter_string=f\"attributes.timestamp_ms > {ten_minutes_ago} AND \"\n",
    "                 f\"attributes.status = 'OK' AND \"\n",
    "                 f\"tags.`mlflow.traceName` = 'generate_sales_email'\",\n",
    "    order_by=[\"attributes.timestamp_ms DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(traces)} successful traces from beta test\")\n",
    "\n",
    "# 3. Add the traces to the evaluation dataset\n",
    "eval_dataset.merge_records(traces)\n",
    "print(f\"Added {len(traces)} records to evaluation dataset\")\n",
    "\n",
    "# Preview the dataset\n",
    "df = eval_dataset.to_df()\n",
    "print(f\"\\nDataset preview:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(\"\\nSample records:\")\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1d4c71-d292-4077-81a6-352994e714fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluation"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import (\n",
    "    RetrievalGroundedness,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    "    Guidelines,\n",
    ")\n",
    "\n",
    "# Save the scorers as a variable so we can re-use them in step 7\n",
    "email_scorers = [\n",
    "        RetrievalGroundedness(),  # Checks if email content is grounded in retrieved data\n",
    "        Guidelines(\n",
    "            name=\"follows_instructions\",\n",
    "            guidelines=\"The generated email must follow the user_instructions in the request.\",\n",
    "        ),\n",
    "        Guidelines(\n",
    "            name=\"concise_communication\",\n",
    "            guidelines=\"The email MUST be concise and to the point. The email should communicate the key message efficiently without being overly brief or losing important context.\",\n",
    "        ),\n",
    "        Guidelines(\n",
    "            name=\"mentions_contact_name\",\n",
    "            guidelines=\"The email MUST explicitly mention the customer contact's first name (e.g., Alice, Bob, Carol) in the greeting. Generic greetings like 'Hello' or 'Dear Customer' are not acceptable.\",\n",
    "        ),\n",
    "        Guidelines(\n",
    "            name=\"professional_tone\",\n",
    "            guidelines=\"The email must be in a professional tone.\",\n",
    "        ),\n",
    "        Guidelines(\n",
    "            name=\"includes_next_steps\",\n",
    "            guidelines=\"The email MUST end with a specific, actionable next step that includes a concrete timeline.\",\n",
    "        ),\n",
    "        RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "        Safety(),  # Checks for harmful or inappropriate content\n",
    "    ]\n",
    "\n",
    "# Run evaluation with predefined scorers\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=df, # Needs to be a pandas df, not spark\n",
    "    predict_fn=generate_sales_email,\n",
    "    scorers=email_scorers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972e5ddd-16fd-45d5-9065-138df1cea3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "___\n",
    "# Results\n",
    "To see the results of the evaluation, you can check out the UI for a more visual presentation by clicking the \"View evaluation results\" button generated by the cell above, or for a more programatic approach, you can search for it using mlflow.search_traces.\n",
    "\n",
    "In the UI, it is also very easy to compare two different evaluations (between two distinct models, for instance) to assertain which achieved better results for your tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3052aa5-1b8e-4548-91ad-bb14554f0313",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Results"
    }
   },
   "outputs": [],
   "source": [
    "eval_traces = mlflow.search_traces(run_id=eval_results.run_id)\n",
    "\n",
    "# eval_traces is a Pandas DataFrame that has the evaluated traces.  The column `assessments` includes each scorer's feedback.\n",
    "eval_traces.head(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6666829028662755,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "example_mlflow3_evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
